<!DOCTYPE html>
<html lang="en" dir="ltr"><head>
  
                           
     


<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.83.1" />
<title>Deep Reinforcement Learning-WindFarm | Nadhir Hassen</title>


<meta property="twitter:site" content="@spcanelon">
<meta property="twitter:creator" content="@spcanelon">







  
    
  
<meta name="description" content="Human-level control through deep reinforcement learning">


<meta property="og:site_name" content="Nadhir Hassen">
<meta property="og:title" content="Deep Reinforcement Learning-WindFarm | Nadhir Hassen">
<meta property="og:description" content="Human-level control through deep reinforcement learning" />
<meta property="og:type" content="page" />
<meta property="og:url" content="https://nadhirhass.netlify.app/blog/deep-learning/" />
<meta property="og:locale" content="en">




    
        <meta property="og:image" content="https://nadhirhass.netlify.app/blog/deep-learning/featured.png" >
        <meta property="twitter:card" content="summary_large_image">
        <meta name="twitter:image" content="https://nadhirhass.netlify.app/blog/deep-learning/featured.png" >
    
    
  <meta itemprop="name" content="Deep Reinforcement Learning-WindFarm">
<meta itemprop="description" content="Reinforcement learning is known to be unstable or even to diverge when a nonlinear function approximator such as a neural network is used to represent the action-value known as Q function."><meta itemprop="datePublished" content="2022-02-01T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2022-02-01T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="715"><meta itemprop="image" content="https://nadhirhass.netlify.app/blog/deep-learning/featured.png">
<meta itemprop="keywords" content="Bayesian,DRL,Dynamical Systems,Python," />
  
  <!--[if IE]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  <link rel="shortcut icon" href="/img/logo_rounded.ico" type="image/x-icon">
  <link rel="icon" href="/img/logo_rounded.ico" type="image/x-icon">
  
  
  <link rel="stylesheet" href="/style.main.min.c9a40da5612a51ebf5a49a932368f95bdbf3cdb647758b9cbb66393f2a752e50.css" integrity="sha256-yaQNpWEqUev1pJqTI2j5W9vzzbZHdYucu2Y5Pyp1LlA=" media="screen">
  
  
  <script src="/panelset.min.d74e921a1b9af2d938fdff19e433ba539cdb202961eddae2356a54199f0653ec.js" type="text/javascript"></script>
  
  
  <script src="/main.min.38a0323c5b0bbb611c4874ba2d8fdaba57d20cc2b0d704b30250c235ba8b6d49.js" type="text/javascript"></script>
  
  
  <script src="/toc.min.f73cb355a2cb0aa2ae5f3f9693cfcaa76280e8a97ccfd2290c3cd514ee82f177.js" type="text/javascript"></script>
</head>
<body>
      <div class="grid-container single-sidebar">
<header class="site-header pt4 pb2 mb4 bb b--transparent ph5 headroom z-max" role="banner">
  <nav class="site-nav db dt-l w-100" role="navigation">
    <a class="site-brand db dtc-l v-mid link no-underline w-100 w-33-l tc tl-l" href="https://nadhirhass.netlify.app/" title="Home">
      <img src="/img/logo_noBg.svg" class="dib db-l h2 w-auto" alt="Nadhir Hassen">
    </a>
    <div class="site-links db dtc-l v-mid w-100 w-47-l tc tr-l mt3 mt0-l ttu tracked">
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/about/" title="About Me">About</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 active" href="/blog/" title="Collaborating With People">Blog</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/talk/" title="Talks and Presentations">Talks</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/publication/" title="Research Publications">Publications</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/project/" title="Project Portfolio">Projects</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/teaching/" title="Teaching Portfolio">Teaching</a>
      
      
    </div>
  </nav>
</header>

<main class="page-main pa4" role="main">
  <section class="page-content mw7 center">
    <article class="post-content pa0 pr3-l">
      <header class="post-header">
        <h1 class="f1 lh-solid measure-narrow mb3 fw4">Deep Reinforcement Learning-WindFarm</h1>
        <h2 class="f4 mt0 mb4 lh-title measure">Human-level control through deep reinforcement learning</h2>
        <p class="f6 measure lh-copy mv1">By Nadhir Hassen in <a href="https://nadhirhass.netlify.app/categories/deep-learning">Deep Learning</a>  <a href="https://nadhirhass.netlify.app/categories/reinforcement-learning">Reinforcement Learning</a>  <a href="https://nadhirhass.netlify.app/categories/hyperparameters-optimization">Hyperparameters Optimization</a>  <a href="https://nadhirhass.netlify.app/categories/wind-farm">Wind Farm</a> </p>
        <p class="f7 db mv0 ttu">February 1, 2022</p>
      
        <div class="ph0 pt5">
          
    
    
    
      
    
    
    
    
    
      
      
  <a class="btn-links mr2 ba dib" href="https://github.com/vincehass/Deep-Reinforcement-Learning-Optimal-Control" target="_blank" rel="noopener"><i class="fab fa-github fa-lg fa-fw mr2"></i>My code repository Project</a>


        </div>
      
      </header>
      <section class="post-body pt5 pb4">
        <script src="https://nadhirhass.netlify.app/blog/deep-learning/index_files/clipboard/clipboard.min.js"></script>
<link href="https://nadhirhass.netlify.app/blog/deep-learning/index_files/xaringanExtra-clipboard/xaringanExtra-clipboard.css" rel="stylesheet" />
<script src="https://nadhirhass.netlify.app/blog/deep-learning/index_files/xaringanExtra-clipboard/xaringanExtra-clipboard.js"></script>
<script>window.xaringanExtraClipboard(null, {"button":"<i class=\"fa fa-clipboard\"><\/i> Copy Code","success":"<i class=\"fa fa-check\" style=\"color: #90BE6D\"><\/i> Copied!","error":"Press Ctrl+C to Copy"})</script>
<link href="https://nadhirhass.netlify.app/blog/deep-learning/index_files/font-awesome/css/all.css" rel="stylesheet" />
<link href="https://nadhirhass.netlify.app/blog/deep-learning/index_files/font-awesome/css/v4-shims.css" rel="stylesheet" />
<style type="text/css">
.page-main img {
  box-shadow: 0px 0px 2px 2px rgba( 0, 0, 0, 0.2 );
}
</style>




<h2 id="introduction">Introduction
  <a href="#introduction" title="Link to heading"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>We investigate and improve Reinforcement Learning  instability which causes: the correlations present in the sequence of observations, the fact that small updates to $Q$ may significantly change the policy and therefore change the data distribution, and the correlations between the action-values ($Q$) and the target values $r + \gamma \max_{a'} Q(s', a')$.</p>
<!-- <i class="fas fa-glass-cheers pr2"></i>  -->
<!-- ```python
porridge = "blueberry"
if porridge == "blueberry":
    print("Eating...")
``` -->
<!-- <details><summary>Projects on my site: https://nadhirhass.rbind.io/project</summary>
<div class="figure" style="text-align: center">
<img src="img/silvia-project-list.png" alt="The Project listing page for my site with a grid layout featuring thumbnail images. The top of the page says 'Projects' and below is a short description of what can be found on the page. It also includes a by-line that reads 'Written by Silvia CanelÃ³n.' There are three projects featured with a decorative thumbnail image, a title, and a summary. Items also include an author and category links but they are cut off in this screenshot." width="1265" />
<p class="caption">Figure 1: My Project listing: https://silvia.rbind.io/project</p>
</div>
</details> -->
<p>The authors suggest two key ideas to address these instabilities with a novel variant of Q-learning: Replay buffer and Fixed Q-target.</p>




<h2 id="uniformly-random-sampling-from-experience-replay-memory">Uniformly random sampling from Experience Replay Memory
  <a href="#uniformly-random-sampling-from-experience-replay-memory" title="Link to heading"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>Reinforcement learning agent stores the experiences consecutively in the buffer, so adjacent ($s, a, r, s'$) transitions stored are highly likely to have correlation. To remove this, the agent samples experiences uniformly at random from the pool of stored samples $\big( (s, a, r, s') \sim U(D) \big)$. See sample_batch method of ReplayBuffer class for more details.</p>




<h2 id="fixed-q-target">Fixed Q-target
  <a href="#fixed-q-target" title="Link to heading"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>DQN uses an iterative update that adjusts the action-values ($Q$) towards target values that are only periodically updated, thereby reducing correlations with the target; if not, it is easily divergy because the target continuously moves. The Q-learning update at iteration $i$ uses the following loss function:</p>
<p>$$
\begin{equation}
\begin{split}
L_i(\theta_i) = \mathbb{E}_{(s,a,r,s')}\big[ \big( r + \gamma \max _{a'} Q(s',a';\theta_i^-) - Q(s, a; \theta_i) \big)^2 \big]
\end{split}
\end{equation}
$$</p>
<p>in which $\gamma$ is the discount factor determining the agentâs horizon, $\theta_i$ are the parameters of the Q-network at iteration $i$ and $\theta_i^-$ are the network parameters used to compute the target at iteration $i$. The target network parameters $\theta_i^-$ are only updated with the Q-network parameters ($\theta_i$) every C steps and are held fixed between individual updates. ($C = 200$ in CartPole-v0)</p>




<h3 id="for-more-stability-gradient-clipping">For more stability: Gradient clipping
  <a href="#for-more-stability-gradient-clipping" title="Link to heading"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>The authors also found it helpful to clip the error term from the update $r + \gamma \max_{a'} Q(s', a'; \theta_i^-) - Q(s,a,;\theta_i)$ to be between -1 and 1. Because the absolute value loss function $|x|$ has a derivative of -1 for all negative values of x and a derivative of 1 for all positive values of x, clipping the squared error to be between -1 and 1 corresponds to using an absolute value loss function for errors outside of the (-1,1) interval. This form of error clipping further improved the stability of the algorithm.</p>




<h2 id="benchmark-results">Benchmark Results
  <a href="#benchmark-results" title="Link to heading"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>




<h3 id="1-environements-cartpole-and-pendulum-for-classic-control-and-robotics">1. Environements: CartPole and Pendulum for Classic Control and Robotics
  <a href="#1-environements-cartpole-and-pendulum-for-classic-control-and-robotics" title="Link to heading"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>Below shows various RL algorithms successfully learning discrete action game 
<a href="https://gym.openai.com/envs/#classic_control" target="_blank" rel="noopener">Cart Pole</a>
or continuous action game 
<a href="https://gym.openai.com/envs/#classic_control" target="_blank" rel="noopener">Pendulum</a>. We record the average result from running the algorithms with 3 random seeds is shown with the shaded area representing plus and minus 1 standard deviation.</p>




<h3 id="2-policy-gradients-algorithm-experiements">2. Policy Gradients Algorithm Experiements
  <a href="#2-policy-gradients-algorithm-experiements" title="Link to heading"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>Below shows the performance of Actor Critic models such as DDPG, PPO, SAC and TD3 including learning acceleration methods using demonstrations for treating real applications with sparse rewards.</p>
<p>The results replicate the results found in the papers. In the next stage, I plan to show how adding HER can allow an agent to solve problems that it otherwise would not be able to solve at all. Note that the same hyperparameters were used within each pair of agents and so the only difference between them was whether hindsight was used or not.</p>
<p><img src="DQN_PER-CartPole-v1-2022-03-09.png" alt="Individual Cart Pole Results for DDPG"> 
<img src="SAC-Pendulum-v1-2022-03-09.png" alt="Individual Cart Pole Results for SAC"></p>




<h3 id="3-dqn-learning-algorithm-experiments">3. DQN Learning Algorithm Experiments
  <a href="#3-dqn-learning-algorithm-experiments" title="Link to heading"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>The results of DQN&rsquo;s show how we could avoid instable or even divergent nonlinear function approximator presented in the action-value Q function. The instability is more often caused by the presence of correlation in the sequence of observations, 
<a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf" target="_blank" rel="noopener">DQN</a> suggest two key ideas to address these instabilities with a novel variant of Q-learning: Replay buffer and Fixed Q-target.</p>
<p>The results replicate the results found in the papers for DQN, DoubleDQN, PrioritizedExperienceReplay and N-stepLearning.</p>
<p><img src="DQN_agents.png" alt="2021 PSB Poster (letter)"></p>




<h2 id="windfarm-environements-for-active-wake-control">WindFarm Environements for Active Wake Control
  <a href="#windfarm-environements-for-active-wake-control" title="Link to heading"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>Below shows various RL algorithms successfully learning discrete action. We record the average result from running the algorithms with 4 random seeds is shown with the shaded area representing plus and minus 1 standard deviation. Hyperparameters used are large and can be optimized.</p>
<p><img src="Noisy_result.png" alt="General Reward Performance Results for SAC, Noisy and Floris Algorithms"></p>
<!-- 7. <i class="fas fa-glass-cheers pr2"></i>Celebrate and share your brand new site! ð ð¥³ ð¾<br>If you share on Twitter, use the #HugoApero hashtag so the [Hugo ApÃ©ro squad](https://twitter.com/apreshill/status/1397052533588185092) can clap for you! -->

      </section>
      <footer class="post-footer">
        <div class="post-pagination dt w-100 mt4 mb2">
  
  
    <a class="next dtc pl2 tr v-top fw6"
    href="https://nadhirhass.netlify.app/blog/nodes/">Neural ODEs &rarr;</a>
  
</div>

      </footer>
    </article>
    
      
<div class="post-comments pa0 pa4-l mt4">
  
  <script src="https://utteranc.es/client.js"
          repo="spcanelon/silvia"
          issue-term="title"
          theme="boxy-light"
          label="comments :crystal_ball:"
          crossorigin="anonymous"
          async
          type="text/javascript">
  </script>
  
</div>

    
  </section>
</main>
<aside class="page-sidebar" role="complementary">
                         
 


                       
 











  <img src="/blog/sidebar.jpg" class="db ma0" alt="">



<div class="blog-info ph4 pt4 pb4 pb0-l">
  

  <h1 class="f3">Collaborating With People about Their Passion</h1>
  <p class="f6 lh-copy measure">This is my blog where I practice sharing my curiosity about Machine Learning. It includes notes and tutorials for my future self and hopefully also for you.</p>
  <p class="f7 measure lh-copy i mh0-l">Written by Nadhir Hassen</p>


  <small class="db f7"><a href="/blog/" class="dib fw7 ttu bt bw1 b--black-10 pt3">View recent posts</a></small>
</div>


  
  
  
<details closed class="f6 fw7 input-reset">
  <dl class="f6 lh-copy">
    <dt class="fw7">Posted on:</dt>
    <dd class="fw5 ml0">February 1, 2022</dd>
  </dl>
  <dl class="f6 lh-copy">
    <dt class="fw7">Length:</dt>
    <dd class="fw5 ml0">4 minute read, 715 words</dd>
  </dl>
  
  <dl class="f6 lh-copy">
    <dt class="fw7">Categories:</dt>
    <dd class="fw5 ml0"> <a href="https://nadhirhass.netlify.app/categories/deep-learning">Deep Learning</a>  <a href="https://nadhirhass.netlify.app/categories/reinforcement-learning">Reinforcement Learning</a>  <a href="https://nadhirhass.netlify.app/categories/hyperparameters-optimization">Hyperparameters Optimization</a>  <a href="https://nadhirhass.netlify.app/categories/wind-farm">Wind Farm</a> </dd>
  </dl>
  
  
  
  <dl class="f6 lh-copy">
    <dt class="fw7">Tags:</dt>
    <dd class="fw5 ml0"> <a href="https://nadhirhass.netlify.app/tags/bayesian">Bayesian</a>  <a href="https://nadhirhass.netlify.app/tags/drl">DRL</a>  <a href="https://nadhirhass.netlify.app/tags/dynamical-systems">Dynamical Systems</a>  <a href="https://nadhirhass.netlify.app/tags/python">Python</a> </dd>
  </dl>
  
  <dl class="f6 lh-copy">
    <dt class="fw7">See Also:</dt>
    
    <dd class="fw5 ml0"><a href="/blog/nodes/">Neural ODEs</a></dd>
    
    <dd class="fw5 ml0"><a href="/blog/sde-epidemic/">Stochastic differential equations: Application to epidemiology</a></dd>
    
    <dd class="fw5 ml0"><a href="/teaching/projects/ml/">Probability and Statistics</a></dd>
    
  </dl>
</details>

                         



<nav id="TableOfContents" class="sticky ph4 pb4 pt6" role="navigation">
  <h2 class="mv0 f5 fw7 ttu tracked dib">On this page</h2>
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#introduction">Introduction</a></li>
        <li><a href="#uniformly-random-sampling-from-experience-replay-memory">Uniformly random sampling from Experience Replay Memory</a></li>
        <li><a href="#fixed-q-target">Fixed Q-target</a></li>
        <li><a href="#benchmark-results">Benchmark Results</a></li>
        <li><a href="#windfarm-environements-for-active-wake-control">WindFarm Environements for Active Wake Control</a></li>
      </ul>
    </li>
  </ul>
</nav>
</nav>


</aside>
<footer class="site-footer pv4 bt b--transparent ph5" role="contentinfo">
    <nav class="db dt-l w-100">
    <p class="site-copyright f7 db dtc-l v-mid w-100 w-33-l tc tl-l pv2 pv0-l mv0 lh-copy">
      &copy; 2023 Nadhir Hassen
      <span class="middot-divider"></span>
      Made with <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"><a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/hugo-apero/" rel="dct:source">Hugo ApÃ©ro</a></span>.
      <br />
      
Based on <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"><a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/formspree/blogophonic-hugo" rel="dct:source">Blogophonic</a></span> by <a xmlns:cc="http://creativecommons.org/ns#" href="https://formspree.io" property="cc:attributionName" rel="cc:attributionURL">Formspree</a>.
    </p>
    
    <div class="site-social-links db dtc-l v-mid w-100 w-33-l tc pv2 pv0-l mv0">
      <div class="social-icon-links" aria-hidden="true">
  
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="/accessibility/" title="universal-access" >
      <i class="fas fa-universal-access fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://twitter.com/NadeerAct" title="twitter" target="_blank" rel="noopener">
      <i class="fab fa-twitter fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://github.com/vincehass" title="github" target="_blank" rel="noopener">
      <i class="fab fa-github fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://linkedin.com/in/nadhir-vincent-hass-216391aa" title="linkedin" target="_blank" rel="noopener">
      <i class="fab fa-linkedin fa-lg fa-fw"></i>
    </a>
  
</div>

    </div>
    
    <div class="site-links f6 db dtc-l v-mid w-100 w-67-l tc tr-l pv2 pv0-l mv0">
      
      <a class="dib pv1 ph2 link" href="/accessibility/" title="Accessibility Commitment">Accessibility</a>
      
      <a class="dib pv1 ph2 link" href="/contact/" title="Contact Form">Contact</a>
      
      <a class="dib pv1 ph2 link" href="/license/" title="License Details">License</a>
      
      <a class="dib pv1 ph2 link" href="/blog/index.xml/" title="Subscribe via RSS">RSS</a>
      
    </div>
  </nav>
    <script src="//yihui.org/js/math-code.js"></script>
    <script async
      src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
    
<script type="text/javascript"
        src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</footer>
      </div>
    </body>
</html>
