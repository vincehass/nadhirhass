<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Gaussian Processes on Nadhir Hassen</title>
    <link>https://nadhirhass.netlify.app/tags/gaussian-processes/</link>
    <description>Recent content in Gaussian Processes on Nadhir Hassen</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 31 Dec 2021 09:30:00 +0000</lastBuildDate><atom:link href="https://nadhirhass.netlify.app/tags/gaussian-processes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Approximate Bayesian Neural Networks</title>
      <link>https://nadhirhass.netlify.app/talk/2021-12-14-bnn-slides/</link>
      <pubDate>Fri, 31 Dec 2021 09:30:00 +0000</pubDate>
      
      <guid>https://nadhirhass.netlify.app/talk/2021-12-14-bnn-slides/</guid>
      <description>Descriptionwe address these issues by attempting to demystify the relationship between approximate inference and optimization approaches through the generalized Gauss–Newton method. Bayesian deep learning yields good results, combining Gauss–Newton with Laplace and Gaussian variational approximation. Both methods compute a Gaussian approximation to the posterior; however, it remains unclear how these methods affect the underlying probabilistic model and the posterior approximation. Both methods allow a rigorous analysis of how a particular model fails and the ability to quantify its uncertainty.</description>
    </item>
    
    <item>
      <title>Approximate Bayesian Optimisation for Neural Networks</title>
      <link>https://nadhirhass.netlify.app/publication/bo_project/</link>
      <pubDate>Thu, 01 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://nadhirhass.netlify.app/publication/bo_project/</guid>
      <description>A novel Bayesian Optimization method based on a linearized link-function to accounts the under-presented class by using a GP surrogate model. This method is based on Laplace’s method and Gauss-Newton approximations to the Hessian. Our method can improve generalization and be useful when validation data is unavailable (e.g., in nonstationary settings) to solve heteroscedastic behaviours. Our experiments demonstrate that our BO by Gauss-Newton approach competes favorably with state-of-the-art blackbox optimization algorithms.</description>
    </item>
    
    <item>
      <title>Kronecker-factored approximation (KFAC) of the Laplace-GGN for Continual Learning</title>
      <link>https://nadhirhass.netlify.app/publication/continual-learning/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://nadhirhass.netlify.app/publication/continual-learning/</guid>
      <description>Publication highlighting how Catastrophic Forgetting can be solved by Laplace Gauss-Newton approximation without alternating the complexity cost.</description>
    </item>
    
    <item>
      <title>Orion-Asynchronous Distributed Hyperparameter Optimization</title>
      <link>https://nadhirhass.netlify.app/publication/orion_project/</link>
      <pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://nadhirhass.netlify.app/publication/orion_project/</guid>
      <description>Oríon is a black-box function optimization library with a key focus on usability and integrability for its users. As a researcher, you can integrate Oríon to your current workflow to tune your models but you can also use Oríon to develop new optimization algorithms and benchmark them with other algorithms in the same context and conditions.</description>
    </item>
    
  </channel>
</rss>
